services:
  gpu:
    container_name: ml_gpu
    image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
    command: bash /tool/start.sh
    ports:
      - "2222:22"
    runtime: nvidia
    volumes:
      - ./tool:/tool
      # if you need to attach a volume, you can add it here
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD","nvidia-smi", "--query-gpu=memory.used", "--format=csv,noheader,nounits"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s